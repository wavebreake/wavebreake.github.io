---
layout: post
title:  "fio安装与测试"
date:   2021-09-01 12:00:00 +0800
categories: linux

---

# 一、安装部署

## 1. fio安装

在线安装：yum install -y fio

离线安装：

tar -zxvf fio.tar.bz

cd fio

rpm -ivh *

## 2. libaio引擎安装

安装libaio引擎：

在线安装：yum install -y libaio

离线安装：

tar -zxvf libaio.tar.gz

rpm -ivh ibaio/*

优先选择libaio引擎测试

libaio时Linux下原生异步IO接口，用此引擎测试更加准确。

# 二、测试环境

系统版本：CentOS 7.6

文件系统：xfs

主机配置：2C 2G

IO引擎：libaio

测试工具：fio-3.7

# 三、测试目的

用fio磁盘测试工具，调节各个参数的变化，模拟出实际业务状态下磁盘的运行、性能状况；

测试数据在连续长时间读写、bs块大小、目录深度和并发数量变化情况下对磁盘的读写速度磁盘使用率及IOPS等性能影响情况。

runtime： 测试时间长度，可模拟磁盘连续长时间读写

bs： 模拟单次io的块文件大小

iodepth： 模拟IO阻塞情况下磁盘读写性能变化

numjobs： 模拟并发量较高情况下磁盘读写性能变化

# 四、测试模式

会分别测试各个参数下**顺序读**、**顺序写**、**随机读**、**随机写**的BW和IOPS情况和磁盘利用率。

# 五、测试对象

## 1 云硬盘（普通IO）

### 1.1 runtime

**测试内容：**

测试**4k随机读写**和**64k顺序读写**的过程中，不同的runtime所得到的测试结果，观察从多长时间开始可以得到稳定的测试结果。

**测试脚本**

runtime.fio.conf：

```
[global]
ioengine=libaio      # 异步IO
direct=1             # 排除OS的IO缓存机制的影响
size=5g              # 每个fio进程/线程的最大读写
#lockmem=1G           # 锁定所使用的内存大小
directory=/root/fio-disktest   # XFS格式的磁盘，未直接用裸盘
iodepth=1            # 队列深度1（后续有关于参数的测试）
numjobs=1            # 同时开启的fio进程/线程数为1
rw=randread          # 每次测试修改该值：randread/read/randwrite/write
bs=4k                # 每次测试修改该值：rand对应4k，seq对应64k

[job]
runtime=10           # 本次测试runtime：10 20 30 40 50 60 90
```

**测试命令：**

```
mkdir -p logs/{runtime.4k_randread,runtime.4k_randwrite,runtime.64k_read,runtime.64k_write}
# 修改不同的rw和bs值，然后执行4次如下命令（注意修改tee输出的log目录）
for runtime in 10 20 30 40 50 60 90; do sed -i "/^runtime/c runtime=${runtime}" fio.conf && fio fio.conf | tee logs/runtime.4k_randread/${runtime}.log && sleep 10s; done
```

**测试结果：**

结果包含“IOPS”“BW”等

```
runtime.4k_randread/10.log       read: IOPS=1540, BW=6162KiB/s (6310kB/s)(60.2MiB/10005msec)
runtime.4k_randread/20.log       read: IOPS=1522, BW=6089KiB/s (6235kB/s)(119MiB/20001msec)
runtime.4k_randread/30.log       read: IOPS=1442, BW=5770KiB/s (5909kB/s)(169MiB/30001msec)
runtime.4k_randread/40.log       read: IOPS=1430, BW=5721KiB/s (5858kB/s)(223MiB/40001msec)
runtime.4k_randread/50.log       read: IOPS=1473, BW=5895KiB/s (6037kB/s)(288MiB/50001msec)
runtime.4k_randread/60.log       read: IOPS=1494, BW=5980KiB/s (6123kB/s)(350MiB/60001msec)
runtime.4k_randread/90.log       read: IOPS=1465, BW=5861KiB/s (6001kB/s)(515MiB/90001msec)

runtime.4k_randwrite/10.log      write: IOPS=820, BW=3281KiB/s (3360kB/s)(32.1MiB/10003msec)
runtime.4k_randwrite/20.log      write: IOPS=808, BW=3234KiB/s (3311kB/s)(63.2MiB/20001msec)
runtime.4k_randwrite/30.log      write: IOPS=797, BW=3192KiB/s (3268kB/s)(93.5MiB/30007msec)
runtime.4k_randwrite/40.log      write: IOPS=796, BW=3185KiB/s (3262kB/s)(124MiB/40003msec)
runtime.4k_randwrite/50.log      write: IOPS=805, BW=3221KiB/s (3298kB/s)(157MiB/50006msec)
runtime.4k_randwrite/60.log      write: IOPS=846, BW=3386KiB/s (3467kB/s)(198MiB/60001msec)
runtime.4k_randwrite/90.log      write: IOPS=845, BW=3383KiB/s (3464kB/s)(297MiB/90002msec)

runtime.64k_read/10.log       read: IOPS=2265, BW=142MiB/s (148MB/s)(1416MiB/10001msec)
runtime.64k_read/20.log       read: IOPS=2401, BW=150MiB/s (157MB/s)(2048MiB/13643msec)
runtime.64k_read/30.log       read: IOPS=2419, BW=151MiB/s (159MB/s)(2048MiB/13543msec)
runtime.64k_read/40.log       read: IOPS=2358, BW=147MiB/s (155MB/s)(2048MiB/13892msec)
runtime.64k_read/50.log       read: IOPS=2379, BW=149MiB/s (156MB/s)(2048MiB/13771msec)
runtime.64k_read/60.log       read: IOPS=2383, BW=149MiB/s (156MB/s)(2048MiB/13745msec)
runtime.64k_read/90.log       read: IOPS=2411, BW=151MiB/s (158MB/s)(2048MiB/13586msec)

runtime.64k_write/10.log      write: IOPS=601, BW=37.6MiB/s (39.4MB/s)(376MiB/10007msec)
runtime.64k_write/20.log      write: IOPS=730, BW=45.7MiB/s (47.9MB/s)(914MiB/20001msec)
runtime.64k_write/30.log      write: IOPS=847, BW=52.9MiB/s (55.5MB/s)(1588MiB/30002msec)
runtime.64k_write/40.log      write: IOPS=841, BW=52.6MiB/s (55.2MB/s)(2048MiB/38937msec)
runtime.64k_write/50.log      write: IOPS=851, BW=53.2MiB/s (55.8MB/s)(2048MiB/38468msec)
runtime.64k_write/60.log      write: IOPS=890, BW=55.6MiB/s (58.3MB/s)(2048MiB/36816msec)
runtime.64k_write/90.log      write: IOPS=867, BW=54.2MiB/s (56.8MB/s)(2048MiB/37781msec)
```

**测试结果统计图：**

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/runtime-4k-ran-read-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/runtime-4k-ran-write-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/runtime-64k-seq-read-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/runtime-64k-seq-write-bw%26iops.png)

**结论：**

在实际测试环境中，适当加长测试时间，查看不同文件bs大小在多长时间读写后IO和BW趋于稳定，总结出IOPS上限及最大磁盘吞吐量，以及面对不同块大小在长时间读写时磁盘占用率的变化。

（具体结论待真实环境测试后总结）

**数据取出命令：**

```
[root@wanwan logs]# cat runtime.*/* | grep -e read: -e write:  #取出IOPS BW等数据
[root@wanwan logs]# find runtime.*/*   #取出此时日志的路径及文件名
[root@wanwan logs]# cat runtime.*/* | grep util  | awk '{print $6}' #单独取出磁盘利用率
#ls -alt -r  # 按创建时间倒序排列
#sed -n 'read:/p' filename   也可用于取出数据
```

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/runtime.png)

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/runtime-1.png)

### 1.2 bs

**测试目的：**

观察不同块大小情况会对磁盘读写造成什么样的影响。

**测试内容：**

测试在不同bs 块大小情况下，**顺序读写**和**随机读写**的情况。

**测试脚本：**

```
[global]
ioengine=libaio      # 异步IO
direct=1             # 排除OS的IO缓存机制的影响
size=5g              # 每个fio进程/线程的最大读写
#lockmem=1G           # 锁定所使用的内存大小
runtime=30           # 根据上面的结论以后采用30值
directory=/root/fio-disktest   # XFS格式的磁盘，未直接用裸盘
iodepth=1            # 队列深度1（后续有关于参数的测试）
numjobs=1            # 同时开启的fio进程/线程数为1
rw=randread          # 每次测试修改该值：randread/read/randwrite/write

[job]
bs=4k                # 本次测试bs：1k 2k 4k .. 64m
```

**测试命令：**

```
mkdir -p logs/{bs.randread,bs.randwrite,bs.read,bs.write}
# 修改不同的rw值，然后执行4次如下命令（注意修改tee输出的log目录）
[root@wanwan fio]# for bs in 1k 2k 4k 8k 16k 32k 64k 128k 256k 512k 1m 2m 4m 8m 16m 32m 64m; do sed -i "/^bs/c bs=${bs}" bs.fio.conf && fio bs.fio.conf | tee logs/bs.write/$(date +%T).${bs}.log && sleep 10s; done
```

**数据取出命令：**

```
[root@wanwan logs]# find  bs.*/*
[root@wanwan logs]# cat bs.*/* | grep -e read: -e write:
[root@wanwan logs]# cat bs.*/* | grep util | awk '{print $6}' 
```

**测试结果：**

| 文件名/bs大小                       | IOPS/BW                                                      |
| ------------------------------ | ------------------------------------------------------------ |
| bs.randread/16:10:21.1k.log    | read: IOPS=1546, BW=1546KiB/s (1584kB/s)(45.3MiB/30006msec)  |
| bs.randread/16:11:01.2k.log    | read: IOPS=1523, BW=3048KiB/s (3121kB/s)(89.3MiB/30001msec)  |
| bs.randread/16:11:42.4k.log    | read: IOPS=1471, BW=5885KiB/s (6026kB/s)(172MiB/30011msec)   |
| bs.randread/16:12:22.8k.log    | read: IOPS=1495, BW=11.7MiB/s (12.2MB/s)(350MiB/30001msec)   |
| bs.randread/16:13:03.16k.log   | read: IOPS=1357, BW=21.2MiB/s (22.2MB/s)(636MiB/30001msec)   |
| bs.randread/16:13:43.32k.log   | read: IOPS=1165, BW=36.4MiB/s (38.2MB/s)(1093MiB/30001msec)  |
| bs.randread/16:14:24.64k.log   | read: IOPS=945, BW=59.1MiB/s (61.9MB/s)(1772MiB/30001msec)   |
| bs.randread/16:15:04.128k.log  | read: IOPS=757, BW=94.7MiB/s (99.3MB/s)(2841MiB/30001msec)   |
| bs.randread/16:15:45.256k.log  | read: IOPS=593, BW=148MiB/s (155MB/s)(4449MiB/30001msec)     |
| bs.randread/16:16:25.512k.log  | read: IOPS=302, BW=151MiB/s (159MB/s)(4536MiB/30001msec)     |
| bs.randread/16:17:06.1m.log    | read: IOPS=151, BW=152MiB/s (159MB/s)(4613MiB/30396msec)     |
| bs.randread/16:17:46.2m.log    | read: IOPS=76, BW=152MiB/s (160MB/s)(4650MiB/30514msec)      |
| bs.randread/16:18:27.4m.log    | read: IOPS=38, BW=152MiB/s (160MB/s)(4652MiB/30520msec)      |
| bs.randread/16:19:08.8m.log    | read: IOPS=19, BW=152MiB/s (160MB/s)(4656MiB/30534msec)      |
| bs.randread/16:19:49.16m.log   | read: IOPS=9, BW=153MiB/s (160MB/s)(4656MiB/30520msec)       |
| bs.randread/16:20:30.32m.log   | read: IOPS=4, BW=153MiB/s (161MB/s)(4672MiB/30506msec)       |
| bs.randread/16:21:12.64m.log   | read: IOPS=2, BW=153MiB/s (161MB/s)(4672MiB/30487msec)       |
|                                |                                                              |
| bs.randwrite/16:43:40.1k.log   | write: IOPS=912, BW=912KiB/s (934kB/s)(26.7MiB/30009msec)    |
| bs.randwrite/16:44:21.2k.log   | write: IOPS=910, BW=1821KiB/s (1865kB/s)(53.3MiB/30001msec)  |
| bs.randwrite/16:45:01.4k.log   | write: IOPS=872, BW=3490KiB/s (3574kB/s)(102MiB/30010msec)   |
| bs.randwrite/16:45:42.8k.log   | write: IOPS=776, BW=6214KiB/s (6363kB/s)(182MiB/30001msec)   |
| bs.randwrite/16:46:22.16k.log  | write: IOPS=723, BW=11.3MiB/s (11.9MB/s)(339MiB/30014msec)   |
| bs.randwrite/16:47:03.32k.log  | write: IOPS=641, BW=20.1MiB/s (21.0MB/s)(602MiB/30001msec)   |
| bs.randwrite/16:47:43.64k.log  | write: IOPS=537, BW=33.6MiB/s (35.2MB/s)(1008MiB/30002msec)  |
| bs.randwrite/16:48:24.128k.log | write: IOPS=426, BW=53.3MiB/s (55.9MB/s)(1600MiB/30002msec)  |
| bs.randwrite/16:49:04.256k.log | write: IOPS=326, BW=81.6MiB/s (85.6MB/s)(2449MiB/30001msec)  |
| bs.randwrite/16:49:45.512k.log | write: IOPS=221, BW=111MiB/s (116MB/s)(3319MiB/30004msec)    |
| bs.randwrite/16:50:25.1m.log   | write: IOPS=147, BW=147MiB/s (154MB/s)(4415MiB/30001msec)    |
| bs.randwrite/16:51:06.2m.log   | write: IOPS=75, BW=152MiB/s (159MB/s)(4550MiB/30006msec)     |
| bs.randwrite/16:51:46.4m.log   | write: IOPS=38, BW=152MiB/s (160MB/s)(4616MiB/30310msec)     |
| bs.randwrite/16:52:27.8m.log   | write: IOPS=19, BW=153MiB/s (160MB/s)(4656MiB/30506msec)     |
| bs.randwrite/16:53:08.16m.log  | write: IOPS=9, BW=152MiB/s (160MB/s)(4656MiB/30550msec)      |
| bs.randwrite/16:53:49.32m.log  | write: IOPS=4, BW=153MiB/s (161MB/s)(4672MiB/30505msec)      |
| bs.randwrite/16:54:30.64m.log  | write: IOPS=2, BW=153MiB/s (161MB/s)(4672MiB/30519msec)      |
|                                |                                                              |
| bs.read/15:47:21.1k.log        | read: IOPS=3315, BW=3316KiB/s (3396kB/s)(97.2MiB/30001msec)  |
| bs.read/15:48:02.2k.log        | read: IOPS=3163, BW=6327KiB/s (6479kB/s)(185MiB/30001msec)   |
| bs.read/15:48:42.4k.log        | read: IOPS=3109, BW=12.1MiB/s (12.7MB/s)(364MiB/30001msec)   |
| bs.read/15:49:23.8k.log        | read: IOPS=3678, BW=28.7MiB/s (30.1MB/s)(862MiB/30001msec)   |
| bs.read/15:50:03.16k.log       | read: IOPS=3643, BW=56.9MiB/s (59.7MB/s)(1708MiB/30001msec)  |
| bs.read/15:50:44.32k.log       | read: IOPS=3036, BW=94.9MiB/s (99.5MB/s)(2847MiB/30001msec)  |
| bs.read/15:51:24.64k.log       | read: IOPS=2301, BW=144MiB/s (151MB/s)(4316MiB/30001msec)    |
| bs.read/15:52:05.128k.log      | read: IOPS=1203, BW=150MiB/s (158MB/s)(4514MiB/30001msec)    |
| bs.read/15:52:45.256k.log      | read: IOPS=608, BW=152MiB/s (159MB/s)(4633MiB/30481msec)     |
| bs.read/15:53:26.512k.log      | read: IOPS=304, BW=152MiB/s (160MB/s)(4650MiB/30511msec)     |
| bs.read/15:54:07.1m.log        | read: IOPS=152, BW=152MiB/s (160MB/s)(4650MiB/30513msec)     |
| bs.read/15:54:48.2m.log        | read: IOPS=76, BW=152MiB/s (160MB/s)(4648MiB/30526msec)      |
| bs.read/15:55:29.4m.log        | read: IOPS=38, BW=153MiB/s (160MB/s)(4652MiB/30490msec)      |
| bs.read/15:56:10.8m.log        | read: IOPS=19, BW=153MiB/s (160MB/s)(4656MiB/30531msec)      |
| bs.read/15:56:51.16m.log       | read: IOPS=9, BW=153MiB/s (160MB/s)(4656MiB/30507msec)       |
| bs.read/15:57:32.32m.log       | read: IOPS=4, BW=153MiB/s (161MB/s)(4672MiB/30515msec)       |
| bs.read/15:58:13.64m.log       | read: IOPS=2, BW=153MiB/s (161MB/s)(4672MiB/30490msec)       |
|                                |                                                              |
| bs.write/15:33:24.1k.log       | write: IOPS=1571, BW=1572KiB/s (1609kB/s)(46.0MiB/30002msec) |
| bs.write/15:34:04.2k.log       | write: IOPS=1498, BW=2998KiB/s (3070kB/s)(87.8MiB/30001msec) |
| bs.write/15:34:45.4k.log       | write: IOPS=1451, BW=5805KiB/s (5944kB/s)(170MiB/30001msec)  |
| bs.write/15:35:25.8k.log       | write: IOPS=1350, BW=10.5MiB/s (11.1MB/s)(317MiB/30001msec)  |
| bs.write/15:36:06.16k.log      | write: IOPS=1275, BW=19.9MiB/s (20.9MB/s)(598MiB/30001msec)  |
| bs.write/15:36:46.32k.log      | write: IOPS=1085, BW=33.9MiB/s (35.6MB/s)(1017MiB/30001msec) |
| bs.write/15:37:27.64k.log      | write: IOPS=872, BW=54.5MiB/s (57.2MB/s)(1635MiB/30001msec)  |
| bs.write/15:38:07.128k.log     | write: IOPS=633, BW=79.2MiB/s (83.1MB/s)(2377MiB/30001msec)  |
| bs.write/15:38:48.256k.log     | write: IOPS=419, BW=105MiB/s (110MB/s)(3148MiB/30011msec)    |
| bs.write/15:39:28.512k.log     | write: IOPS=244, BW=122MiB/s (128MB/s)(3662MiB/30004msec)    |
| bs.write/15:40:09.1m.log       | write: IOPS=150, BW=150MiB/s (157MB/s)(4505MiB/30004msec)    |
| bs.write/15:40:49.2m.log       | write: IOPS=75, BW=151MiB/s (158MB/s)(4560MiB/30253msec)     |
| bs.write/15:41:30.4m.log       | write: IOPS=38, BW=152MiB/s (160MB/s)(4652MiB/30509msec)     |
| bs.write/15:42:11.8m.log       | write: IOPS=19, BW=153MiB/s (160MB/s)(4656MiB/30506msec)     |
| bs.write/15:42:52.16m.log      | write: IOPS=9, BW=153MiB/s (160MB/s)(4656MiB/30498msec)      |
| bs.write/15:43:33.32m.log      | write: IOPS=4, BW=153MiB/s (160MB/s)(4672MiB/30553msec)      |
| bs.write/15:44:14.64m.log      | write: IOPS=2, BW=153MiB/s (161MB/s)(4672MiB/30479msec)      |

**测试结果统计图（BW和IOPS）：**

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/bs-4k-seq-read-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/bs-4k-seq-write-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/bs-4k-ran-read-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/bs-4k-ran-write-bw%26iops.png)

**测试结果统计图（BW和磁盘利用率）：**

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/bs-4k-seq-read-bw%26util.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/bs-4k-seq-write-bw%26util.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/bs-4k-ran-read-bw%26util.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/bs-4k-ran-write-bw%26util.png)

**折线图数据取出命令：**

```
cat bs.read/* | grep util | awk '{print($6)}'      #磁盘利用率
cat bs.read/* | grep read: | awk '{print($4)}'        #磁盘读写速度
cat bs.write/* | grep write: | awk '{print($2)}'    #IOPS
```

**结论：**

待天翼云环境上正式测试后总结。

### 1.3 iodepth

**测试目的：**

观察不同iodepth对磁盘读写的影响

**测试内容：**

在随机读写和顺序读写情况下，iodepth不同结果的变化。

**测试脚本：**

```
[global]
ioengine=libaio      # 异步IO
direct=1             # 排除OS的IO缓存机制的影响
size=5g              # 每个fio进程/线程的最大读写
#lockmem=1G           # 锁定所使用的内存大小
runtime=30           # 根据上面的结论以后采用30值
directory=/root/fio-disktest   # XFS格式的磁盘，未直接用裸盘
numjobs=1            # 同时开启的fio进程/线程数为1
rw=randread          # 每次测试修改该值：randread/read/randwrite/write
bs=4k                # 每次测试修改该值：rand对应4k，seq对应64k

[job]
iodepth=1            # 本次测试队列深度：1 2 4 8 16（视情况增加）
```

**测试命令：**

```
mkdir -p logs/{iodepth.4k_randread,iodepth.4k_randwrite,iodepth.64k_read,iodepth.64k_write}
# 修改不同的rw值，然后执行4次如下命令（注意修改tee输出的log目录）
[root@wanwan fio]# for iodepth in 1 2 4 8 16; do sed -i "/^iodepth/c iodepth=${iodepth}" iodepth.fio.conf && fio iodepth.fio.conf | tee logs/iodepth.4k_randread/$(date +%T).${iodepth}.log && sleep 10s; done
```

**测试结果取出命令：**

```
[root@wanwan logs]# find iodepth.*/*   #取出目录文件名
[root@wanwan logs]# cat bs.*/* | grep -e read: -e write:  #取出BW IOPS数据
[root@wanwan logs]# cat bs.*/* | grep util                       #取出磁盘使用率
```

**测试结果：**

| 文件名/测试iodepth数                       | IOPS/BW                                                      | 磁盘利用率                                                                            |
| ------------------------------------ | ------------------------------------------------------------ | -------------------------------------------------------------------------------- |
| iodepth.4k_randread/09:06:47.1.log   | read: IOPS=1555, BW=6224KiB/s (6373kB/s)(182MiB/30001msec)   | vda: ios=46521/28, merge=0/30, ticks=29092/31, in_queue=80, util=0.23%           |
| iodepth.4k_randread/09:07:27.2.log   | read: IOPS=3403, BW=13.3MiB/s (13.9MB/s)(399MiB/30001msec)   | vda: ios=101786/32, merge=0/27, ticks=58157/11, in_queue=29614, util=98.71%      |
| iodepth.4k_randread/09:08:08.4.log   | read: IOPS=5030, BW=19.7MiB/s (20.6MB/s)(590MiB/30016msec)   | vda: ios=150306/32, merge=0/36, ticks=118129/25, in_queue=84237, util=93.88%     |
| iodepth.4k_randread/09:08:48.8.log   | read: IOPS=5107, BW=19.0MiB/s (20.9MB/s)(606MiB/30371msec)   | vda: ios=155108/15, merge=0/35, ticks=236980/1966, in_queue=129021, util=60.22%  |
| iodepth.4k_randread/09:09:29.16.log  | read: IOPS=5082, BW=19.9MiB/s (20.8MB/s)(606MiB/30524msec)   | vda: ios=155183/34, merge=0/40, ticks=467845/11787, in_queue=149224, util=32.35% |
| iodepth.4k_randwrite/09:12:11.1.log  | write: IOPS=897, BW=3589KiB/s (3675kB/s)(105MiB/30009msec)   | vda: ios=0/26854, merge=0/40, ticks=0/29322, in_queue=23, util=0.05%             |
| iodepth.4k_randwrite/09:12:52.2.log  | write: IOPS=1852, BW=7409KiB/s (7586kB/s)(217MiB/30001msec)  | vda: ios=0/55344, merge=0/33, ticks=0/58780, in_queue=29687, util=98.94%         |
| iodepth.4k_randwrite/09:13:32.4.log  | write: IOPS=3767, BW=14.7MiB/s (15.4MB/s)(442MiB/30002msec)  | vda: ios=0/112748, merge=0/32, ticks=0/118043, in_queue=88933, util=99.09%       |
| iodepth.4k_randwrite/09:14:13.8.log  | write: IOPS=5018, BW=19.6MiB/s (20.6MB/s)(588MiB/30002msec)  | vda: ios=1/150121, merge=0/36, ticks=117/234758, in_queue=153031, util=73.13%    |
| iodepth.4k_randwrite/09:14:53.16.log | write: IOPS=5089, BW=19.9MiB/s (20.8MB/s)(606MiB/30486msec)  | vda: ios=0/155150, merge=0/42, ticks=0/475526, in_queue=187069, util=41.02%      |
| iodepth.64k_read/09:22:22.1.log      | read: IOPS=2301, BW=144MiB/s (151MB/s)(4316MiB/30001msec)    | vda: ios=68781/35, merge=0/27, ticks=28957/35, in_queue=39, util=0.05%           |
| iodepth.64k_read/09:23:03.2.log      | read: IOPS=2399, BW=150MiB/s (157MB/s)(4505MiB/30037msec)    | vda: ios=72134/14, merge=0/43, ticks=57789/402, in_queue=22511, util=72.83%      |
| iodepth.64k_read/09:23:43.4.log      | read: IOPS=2438, BW=152MiB/s (160MB/s)(4650MiB/30513msec)    | vda: ios=74394/23, merge=0/33, ticks=117517/8177, in_queue=50791, util=53.24%    |
| iodepth.64k_read/09:24:24.8.log      | read: IOPS=2439, BW=152MiB/s (160MB/s)(4650MiB/30503msec)    | vda: ios=74395/30, merge=0/25, ticks=236077/14064, in_queue=86742, util=39.30%   |
| iodepth.64k_read/09:25:05.16.log     | read: IOPS=2437, BW=152MiB/s (160MB/s)(4650MiB/30523msec)    | vda: ios=74391/38, merge=0/39, ticks=471465/18968, in_queue=165040, util=35.67%  |
| iodepth.64k_write/09:18:25.1.log     | write: IOPS=581, BW=36.3MiB/s (38.1MB/s)(1090MiB/30001msec)  | vda: ios=3/17401, merge=0/43, ticks=0/29470, in_queue=44, util=0.10%             |
| iodepth.64k_write/09:19:05.2.log     | write: IOPS=1166, BW=72.9MiB/s (76.5MB/s)(2188MiB/30002msec) | vda: ios=0/34944, merge=0/43, ticks=0/58604, in_queue=28252, util=94.06%         |
| iodepth.64k_write/09:19:46.4.log     | write: IOPS=2034, BW=127MiB/s (133MB/s)(3816MiB/30002msec)   | vda: ios=2/60874, merge=0/46, ticks=65/115950, in_queue=86781, util=97.09%       |
| iodepth.64k_write/09:20:26.8.log     | write: IOPS=2427, BW=152MiB/s (159MB/s)(4553MiB/30002msec)   | vda: ios=1/72518, merge=0/42, ticks=72/235636, in_queue=158374, util=76.09%      |
| iodepth.64k_write/09:21:07.16.log    | write: IOPS=2408, BW=151MiB/s (158MB/s)(4537MiB/30135msec)   | vda: ios=12/72605, merge=0/56, ticks=137/470607, in_queue=223307, util=50.05%    |

lat是总延迟，slat是提交io到内核的延迟，clat是内核到磁盘完成之间的延迟，因此lat=slat+clat。slat通常是比较稳定的值，所以这里通过观察clat来判断IO延迟情况。

**测试结果统计图：**

**4k随机读：（IOPS、带宽、延迟、磁盘利用率）**

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/iodepth-4k-ran-read-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/iodepth-4k-ran-read-chat%26util.png)

**4k随机写：（IOPS、带宽、延迟、磁盘利用率）**

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/iodepth-4k-ran-write-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/iodepth-4k-ran-write-chat%26util.png)

**64k顺序读:（IOPS、带宽、延迟、磁盘利用率）**

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/iodepth-64k-seq-read-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/iodepth-64k-seq-read-chat%26util.png)

**64k顺序写:（IOPS、带宽、延迟、磁盘利用率）**

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/iodepth-64k-seq-write-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/iodepth-64k-seq-write-chat%26util.png)

**结论：**

### 1.4 numjobs

**测试目的：**

测试不同numjobs对磁盘读写性能的影响。

**测试内容：**

测试随机读写和顺序读写不同numjobs对磁盘各项数据的影响。

**测试脚本：**

```
[global]
ioengine=libaio      # 异步IO
direct=1             # 排除OS的IO缓存机制的影响
size=5g              # 每个fio进程/线程的最大读写
#lockmem=1G           # 锁定所使用的内存大小
runtime=30           # 根据上面的结论以后采用30值
directory=/root/fio-disktest   # XFS格式的磁盘，未直接用裸盘
rw=randread          # 每次测试修改该值：randread/read/randwrite/write
bs=4k                # 每次测试修改该值：rand对应4k，seq对应64k
iodepth=1            # 队列深度固定为1
group_reporting      # 多个job合并出报告

[job]
numjobs=1            # 本次测试numjobs：1 2 4 8 16
```

**测试命令：**

```
mkdir -p logs/{numjobs.4k_randread,numjobs.4k_randwrite,numjobs.64k_read,numjobs.64k_write}
# 修改不同的rw值，然后执行4次如下命令（注意修改tee输出的log目录）
[root@wanwan fio]# for numjobs in 1 2 4 8 16; do sed -i "/^numjobs/c numjobs=${numjobs}" numjobs.fio.conf && fio numjobs.fio.conf | tee logs/numjobs.4k_randread/$(date +%T).${numjobs}.log && sleep 10s; done
```

**测试结果：**

结果中包含 “IOPS” “BW” “磁盘利用率”等

```
numjobs.4k_randread/10:10:09.1.log       read: IOPS=1537, BW=6149KiB/s (6297kB/s)(180MiB/30010msec)    util=0.05%
numjobs.4k_randread/10:10:55.2.log       read: IOPS=3192, BW=12.5MiB/s (13.1MB/s)(374MiB/30001msec)    util=98.00%
numjobs.4k_randread/10:11:41.4.log       read: IOPS=5045, BW=19.7MiB/s (20.7MB/s)(591MiB/30002msec)    util=81.55%
numjobs.4k_randread/10:12:34.8.log       read: IOPS=5094, BW=19.9MiB/s (20.9MB/s)(597MiB/30004msec)    util=50.67%
numjobs.4k_randread/10:13:41.16.log       read: IOPS=5027, BW=19.6MiB/s (20.6MB/s)(605MiB/30790msec)    util=32.17%
numjobs.4k_randwrite/10:16:29.1.log      write: IOPS=869, BW=3477KiB/s (3560kB/s)(102MiB/30001msec)    util=2.40%
numjobs.4k_randwrite/10:17:09.2.log      write: IOPS=1845, BW=7383KiB/s (7560kB/s)(216MiB/30001msec)    util=98.55%
numjobs.4k_randwrite/10:17:50.4.log      write: IOPS=3779, BW=14.8MiB/s (15.5MB/s)(443MiB/30010msec)    util=99.36%
numjobs.4k_randwrite/10:18:30.8.log      write: IOPS=5088, BW=19.9MiB/s (20.8MB/s)(596MiB/30003msec)    util=93.53%
numjobs.4k_randwrite/10:19:11.16.log      write: IOPS=5043, BW=19.7MiB/s (20.7MB/s)(594MiB/30123msec)    util=57.49%
numjobs.64k_read/10:28:17.1.log       read: IOPS=2338, BW=146MiB/s (153MB/s)(1024MiB/7007msec)    util=0.06%
numjobs.64k_read/10:28:35.2.log       read: IOPS=2434, BW=152MiB/s (160MB/s)(2048MiB/13459msec)    util=61.67%
numjobs.64k_read/10:28:59.4.log       read: IOPS=2461, BW=154MiB/s (161MB/s)(4096MiB/26623msec)    util=43.43%
numjobs.64k_read/10:29:36.8.log       read: IOPS=2460, BW=154MiB/s (161MB/s)(4650MiB/30236msec)    util=24.68%
numjobs.64k_read/10:30:16.16.log       read: IOPS=2441, BW=153MiB/s (160MB/s)(4651MiB/30482msec)    util=20.97%
numjobs.64k_write/10:23:40.1.log      write: IOPS=562, BW=35.1MiB/s (36.8MB/s)(1024MiB/29145msec)    util=0.09%
numjobs.64k_write/10:24:19.2.log      write: IOPS=1616, BW=101MiB/s (106MB/s)(2048MiB/20275msec)    util=94.95%
numjobs.64k_write/10:24:50.4.log      write: IOPS=2323, BW=145MiB/s (152MB/s)(4096MiB/28205msec)    util=86.01%
numjobs.64k_write/10:25:29.8.log      write: IOPS=2419, BW=151MiB/s (159MB/s)(4569MiB/30214msec)    util=38.11%
numjobs.64k_write/10:26:09.16.log      write: IOPS=2440, BW=153MiB/s (160MB/s)(4651MiB/30488msec)    util=29.10%
```

**测试结果统计图：**

**4k随机读：（IOPS、带宽、延迟、磁盘利用率）：**

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/numjobs-4k-ran-read-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/numjobs-4k-ran-read-chat%26util.png)

**4k随机写：（IOPS、带宽、延迟、磁盘利用率）：**

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/numjobs-4k-ran-write-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/numjobs-4k-ran-write-chat%26util.png)

**64k顺序读：（IOPS、带宽、延迟、磁盘利用率）：**

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/numjobs-64k-seq-read-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/numjobs-64k-seq-read-chat%26util.png)

**64k顺序写：（IOPS、带宽、延迟、磁盘利用率）：**

![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/numjobs-64k-seq-write-bw%26iops.png)![](https://raw.githubusercontent.com/wavebreake/imagehosting/main/numjobs-64k-seq-write-chat%26util.png)

结论：

# 总结

# 附：

## FIO参数

```
filename=/dev/emcpowerb　支持文件系统或者裸设备，-filename=/dev/sda2或-filename=/dev/sdb
direct=1                 测试过程绕过机器自带的buffer，使测试结果更真实
rw=randwread             测试随机读的I/O
rw=randwrite             测试随机写的I/O
rw=randrw                测试随机混合写和读的I/O
rw=read                  测试顺序读的I/O
rw=write                 测试顺序写的I/O
rw=rw                    测试顺序混合写和读的I/O
bs=4k                    单次io的块文件大小为4k
bsrange=512-2048         同上，提定数据块的大小范围
size=5g                  本次的测试文件大小为5g，以每次4k的io进行测试
numjobs=30               本次的测试线程为30
runtime=1000             测试时间为1000秒，如果不写则一直将5g文件分4k每次写完为止
ioengine=psync           io引擎使用pync方式，如果要使用libaio引擎，需要yum install libaio-devel包
rwmixwrite=30            在混合读写的模式下，写占30%
group_reporting          关于显示结果的，汇总每个进程的信息
lockmem=1g               只使用1g内存进行测试
zero_buffers             用0初始化系统buffer
nrfiles=8                每个进程生成文件的数量
```